# 公众号内容拓展学习笔记（2021.4.1）

------



## :paperclip:  今日要点

1. [推荐 10 个标星 100 K 的 GitHub 开源项目](https://mp.weixin.qq.com/s/M5p-70oh06Mzibni0jb6WQ)         :star::star:
   - 主要内容：10个GitHub高标星的开源项目
   - 我比较喜欢的：**Build Your Own X** 


2. [更稳定的手势识别方法--基于手部骨架与关键点检测](https://mp.weixin.qq.com/s/aKS98nX2Ry7-QaWZoHY6NQ)       :star::star:
   - 主要内容：本期将介绍并演示基于MediaPipe的手势骨架与特征点提取步骤以及以此为基础实现手势识别的方法。
   - 参考阅读：[Google开源手势识别--基于TF Lite/MediaPipe](http://mp.weixin.qq.com/s?__biz=MzU5NDM1MjU5Mg==&mid=2247484065&idx=1&sn=b0945e0ea8c5139e164946c8f29938c7&chksm=fe03c21ec9744b08b03219ef910df10f181c95d091e513cb601fd6f080d9f554bab662e88389&scene=21#wechat_redirect)
   - GitHub：[https://github.com/google/mediapipe](https://github.com/google/mediapipe)


3. [先画轮廓，再涂色，一种新的图像修复方法](https://mp.weixin.qq.com/s/3D2a7ICYT-rFo1alJeBpLQ)       :star::star::star::star::star:
   - 主要内容：EdgeConnect方法，使用GAN来生成边缘，再进一步生成完整的图像。
   - 论文：[EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning](https://arxiv.org/abs/1901.00212)
   - GitHub：[https://github.com/knazeri/edge-connect](https://github.com/knazeri/edge-connect)
   - 核心要点：现有的深度图像修复方法通常会产生模糊的区域。其中一个可能的原因是L1损失的使用(即像素级重建精度)。请注意，**较小的L1损失往往会给出模糊的结果**(最小化平均像素差异)。为了进一步提高修复结果的质量，我们可以向generator network提供缺失区域的先验信息，从而产生更好的局部精细纹理细节。**缺失区域的边缘图可能是一个很好的先验信息的选择，因为它包含了图像的整体结构**。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/KYSDTmOVZvqCfdpeIOTibwsueAJ5TERJZWzwhjco5MXRddwicp1ribbnJ2D5baN49oWCg2NvkpibiaFUmuI5iaXXO2GA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:80%'>
</div>

4. [【语义分割】一篇看完就懂的最新深度语义分割模型综述](https://mp.weixin.qq.com/s/H9KicU9eRbpFMFCvK9y7Tw)       :star::star:
   - 主要内容：很全面的有关语义分割的综述
5. [利用Python 代码实现人体肤色检测](https://mp.weixin.qq.com/s/4IiIm1-2UcgXjdDFLnM0yg)      :star::star:
   - 主要内容：通过OpenCV进行肤色检测
6. [13篇！ 腾讯AI Lab CVPR2021入选论文解读](https://mp.weixin.qq.com/s/pTUzahtdQ5H_4fn41PBNWw)      :star::star:
   - 主要内容：CVPR 2021 接收结果已于本月公布。今年有效投稿多达7500篇，一共有1663篇论文被接收，接收率为27%。其中腾讯 AI Lab 入选13篇，涵盖自监督表征学习、视觉内容生成、多模态理解与生成、3D感知、对抗攻防等方向。以下为论文解读。
7. [【深度学习】CV和NLP通吃！谷歌提出OmniNet：Transformers的全方位表示](https://mp.weixin.qq.com/s/iHAbs3dG7qBte09mqvq-Nw)      :star::star:
   - 主要内容：本文提出了来自Transformer的全方位表示（OmniNet）。
   - 论文：[OmniNet: Omnidirectional Representations from Transformers](https://arxiv.org/pdf/2103.01075.pdf)

## :paperclip:  Others

- 由于图片权限问题，[GitHub](https://github.com/xiaoxuebajie/dairly_learning)是完整版，可以点点 star
- 星标的数量是与个人相关程度，不代表文章内容的好坏
- 关注我的[CSDN](https://mp.csdn.net/console/article)博客
- 关注我的[哔哩哔哩](https://space.bilibili.com/424394389?spm_id_from=333.788.b_765f7570696e666f.1)
- 关注我的公众号CV伴读社

<div align=center><img src="https://img-blog.csdnimg.cn/202005031406335.jpg" style='zoom:80%'>
</div>