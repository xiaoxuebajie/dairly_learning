# 公众号内容拓展学习笔记（2021.4.2）

------



## :paperclip:  今日要点

1. [CVPR2021：推广到开放世界的在线自适应深度视觉里程计](https://mp.weixin.qq.com/s/r_9kjcvW8eqvd-UOFEasUw)         :star::star:
   - 主要内容：一个用于深度VO的在线自适应网络（借助场景不可知的几何计算和贝叶斯推理的辅助）。
   - 论文：[Generalizing to the Open World Deep Visual Odometry with Online Adaptation](https://arxiv.org/abs/2103.15279)
   - 核心要点：
     - 提出一个泛化的深度VO，其使用场景未知几何公式和贝叶斯推断来加速自监督在线自适应性
     - 估计的深度不断被贝叶斯融合网络优化，后续用于训练深度和光流
     - 引入在线学习的深度和光流不确定度以实现精度更高的深度估计和差分高斯牛顿优化。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/Q0FNTB1XHicyicMwPmts0LhUHMUTJypUHGhfkAFHlO4LZ1cjY03otXJZetT7icdNXb55GatpfbwWMJ1Nl02YIWwqw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:50%'>
</div>

2. [【泡泡图灵智库】对抗域特征自适应的夜间图像无监督单目深度估计](https://mp.weixin.qq.com/s/2FSvFj8OHY8g1tcMfOckqg)       :star::star:
   - 主要内容：本文提出了一种基于PatchGAN的域特征自适应方法，用于从无约束的**单目RGB夜间图像中估计深度**。
   - 论文：[Unsupervised Monocular Depth Estimation for Night-time Images using Adversarial Domain Feature Adaptation](https://arxiv.org/abs/2010.01402)

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/Q0FNTB1XHicyicMwPmts0LhUHMUTJypUHGhfkAFHlO4LZ1cjY03otXJZetT7icdNXb55GatpfbwWMJ1Nl02YIWwqw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:50%'>
</div>

3. [Deepfake防御新思路有了！腾讯首次公开MagDR框架，已被AI顶会接收](https://mp.weixin.qq.com/s/A0pAXRmxMyEokAYXay4dbg)        :star::star:
   - 主要内容：首次公开了一种能够消除对抗样本对 Deepfake 干扰攻击的方法，该方法对防止深度伪造能力滥用提出了新思考。同时，也可用于提升 AI 图像处理的安全性。
   - 论文：[MagDR：Mask-guided Detection and Reconstruction for Defending Deepfakes](https://arxiv.org/abs/2103.14211)
   - 核心要点：使用一些非监督性指标对对抗样本在 Deepfake 中所生成的结果进行敏感性评估，并且利用人脸属性区域作为辅助信息以及通过对最优防御方法进行搜索组合的方式检测和重建图片，以期能够达到净化原图并保持 Deepfake 输出真实性的目的。
   - GitHub：[https://github.com/yuanxiaosc/DeepNude-an-Image-to-Image-technology/blob/master/README-ZH.md](https://github.com/yuanxiaosc/DeepNude-an-Image-to-Image-technology/blob/master/README-ZH.md)

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_jpg/ZBjVrHIdkOlvCYfQibPyPR9NuXzjz1gawicS67zwQRfS2xzVPHM2lnK5KADVpBStTlvbeQlyUvPqPe2Bza8iagwSA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:50%'>
</div>

4. [DO-Conv无痛涨点：使用over-parameterized卷积层提高CNN性能](https://mp.weixin.qq.com/s/baVrclTq04J2GAfuxz-Icw)       :star::star::star::star:
   - 主要内容：作者通过在一个普通的卷积层中加入额外的depthwise卷积操作，构成一个over-parameterized的卷积层，并将其命名为DO-Conv，通过实验证明，使用DO-Conv不仅能够加速网络的训练过程，还能在多种计算机视觉任务中取得比使用传统卷积层更好的结果。
   - 论文：[DO-Conv: Depthwise Over-parameterized Convolutional Layer](https://arxiv.org/pdf/2006.12030.pdf)
   - GitHub：[https://github.com/yangyanli/DO-Conv](https://github.com/yangyanli/DO-Conv)
   - 核心要点：
     - 在传统卷积操作中增加额外的参数形成DO-Conv，使用DO-Conv代替传统卷积能够加快收敛速度，在不增加网络推理计算量的前提下提高网络性能
     - 将DO-Conv拓展到DO-DConv和DO-GConv，拓宽其应用范围
     - 通过实验证明了DO-Conv在多种视觉任务中的性能提升
5. [ShuffleNetV2：轻量级CNN网络中的桂冠](https://mp.weixin.qq.com/s/PGr-fYH8kR0zF6_snuaZYg)        :star::star:
   - 主要内容：今天我们要讲的是ShuffleNetv2，它是旷视最近提出的ShuffleNet升级版本，并被ECCV2018收录。在同等复杂度下，ShuffleNetv2比ShuffleNet和MobileNetv2更准确。
   - 论文：[ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design](https://arxiv.org/pdf/1807.11164.pdf)

## :paperclip:  Others

- 由于图片权限问题，[GitHub](https://github.com/xiaoxuebajie/dairly_learning)是完整版，可以点点 star
- 星标的数量是与个人相关程度，不代表文章内容的好坏
- 关注我的[CSDN](https://mp.csdn.net/console/article)博客
- 关注我的[哔哩哔哩](https://space.bilibili.com/424394389?spm_id_from=333.788.b_765f7570696e666f.1)
- 关注我的公众号CV伴读社

<div align=center><img src="https://img-blog.csdnimg.cn/202005031406335.jpg" style='zoom:80%'>
</div>