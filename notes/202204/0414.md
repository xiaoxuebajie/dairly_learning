# 公众号内容拓展学习笔记（2022.4.14）

------



## :paperclip:  今日要点

1. [CVPR 2022｜Self-Attention和CNN的优雅集成！清华大学等提出ACmix，性能速度全面提升！](https://mp.weixin.qq.com/s/rJHR-C4Hos6G9_tL3OCS6w)         :star::star:
   - Abstract: 清华大学等提出ACmix，Self-Attention和CNN的优雅集成，性能速度全面提升
   - Paper: [On the Integration of Self-Attention and Convolution](https://arxiv.org/pdf/2111.14556.pdf)
   - Code: [https://github.com/Panxuran/ACmix](https://github.com/Panxuran/ACmix)
   - Tips:  清华大学等提出了一个混合模型ACmix：它既兼顾Self-Attention和Convolution的优点，同时与Convolution或Self-Attention对应的模型相比，具有更小的计算开销。实验表明，本文方法在图像识别和下游任务上取得了持续改进的结果

<div align=center><img src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfq41B6oVLgoJ5JFgLPjFyVTm3q803luhObhj9CQ09XR2PYmXGfymBQTkrf4Lj4c9MDaw2ibr3ibh0bQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>



2. [CVPR 2022 Oral | MetaFormer：证明Transformer的威力源自其整体架构！颜水成团队工作！](https://mp.weixin.qq.com/s/2WIhkOGKRv4TOlBnaKmEsw)       :star::star:
   - Abstract: MetaFormer：证明Transformer的威力源自其整体架构
   - Paper: [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418)
   - Code: [https://github.com/sail-sg/poolformer](https://github.com/sail-sg/poolformer)
   - Tips: 作者认为`MetaFormer`是为最近的Transformer和类似MLP的视觉任务模型获得优越结果的关键。这项工作需要更多的未来研究，致力于改进`MetaFormer`，而不是专注于`token mixer module`。此外，作者提出的PoolFormer可以作为未来`MetaFormer`设计的Baseline。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/5ooHoYt0tgnITQrfYicacJtGytOgAU3uG510bbx5jTH8ianZCHkmXZEPbFCpvFpgjiaibXGOpAvINRZrokxTv6GDZA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>


3. [视频生成无需GAN、VAE，谷歌用扩散模型联合训练视频、图像，实现新SOTA](https://mp.weixin.qq.com/s/VlQVhBJCzn9yei8ZtWPrWA)       :star::star:
   - Abstract: 谷歌用扩散模型联合训练视频、图像，实现新SOTA
   - Paper: [Video Diffusion Models](https://arxiv.org/pdf/2204.03458.pdf)
   - Code: [https://video-diffusion.github.io/](https://video-diffusion.github.io/)
   - Tips: : 首先谷歌展示了使用扩散模型生成视频的首个结果，包括无条件和有条件设置。其次该研究表明，可以通过高斯扩散模型的标准公式来生成高质量的视频，除了直接的架构更改以适应深度学习加速器的内存限制外，几乎不需要其他修改。
<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gWibJGtFAALicxXwtgjHz4EhbjicVJw0fdbZCGcAI3LOlWEKlPtmbLFs5D120PzzJU4aGfIHqfdpZ9yrA/640?wx_fmt=gif&wxfrom=5&wx_lazy=1" style='zoom:100%'>
</div>


4. [CVPR | 浙大小姐姐提出了秃头生成器，完美保留五官，让程序员简历大加分？](https://mp.weixin.qq.com/s/zurx_rIP2ec5hYPGr4s1PA)       :star::star:
   - Abstract: 浙大小姐姐提出了秃头生成器，完美保留五官
   - Paper: [HairMapper: Removing Hair from Portraits Using GANs](http://www.cad.zju.edu.cn/home/jin/cvpr2022/HairMapper.pdf)
   - Code: [https://github.com/oneThousand1000/non-hair-FFHQ](https://github.com/oneThousand1000/non-hair-FFHQ)
   - Tips: HairMapper的原理一共分为三步，生成秃头→保留五官→合并头像。首先，作者们利用StyleGAN，做出一个与原头型相近的秃头效果；接下来，就是利用InterFaceGAN，抠出一个头发以外的脸型和五官形象，同时也保留四周的风景；最后，将前两步生成的效果合成，就做出了一个完美的“光头”。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDD4P7eGZsOShpoPT09e8H8ribg2Sc9gYtkTm3S2zNLiba5sMUhSry2LtpQib7mn6Bib9Iv6XTVGMdudw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>


5. [CVPR 2022 | 南大提出：Structured Sparse R-CNN：单阶段端到端场景图生成器](https://mp.weixin.qq.com/s/fTT8N7UcrDhyy6CmZ-Fbhw)       :star::star:
   - Abstract: 南大提出：Structured Sparse R-CNN：单阶段端到端场景图生成器
   - Paper: [Structured Sparse R-CNN for Direct Scene Graph Generation](https://arxiv.org/abs/2106.10815)
   - Code: [https://github.com/MCG-NJU/Structured-Sparse-RCNN](https://github.com/MCG-NJU/Structured-Sparse-RCNN)
   - Tips: 本工作将端到端稀疏目标检测器引入场景图生成领域，并提出了相应的关系建模组件和训练策略。该模型在 Visual Genome, Open Image V4/V6 数据集上取得了 SOTA 效果。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_jpg/yNnalkXE7oWyibL6R9xZ20yAfZFAZIMJxzib8tn1b5UPpOicsPo8kNQzlzib3k6VGy1SW45nUFpoc79cPyovyjd63Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>


6. [CVPR 2022｜打破传统的跟踪范式！南大开源MixFormer：端到端目标检测新模型](https://mp.weixin.qq.com/s/VI4SOEsD7VxEaa-PhLgd4g)       :star::star:
   - Abstract: 打破传统的跟踪范式！南大开源MixFormer：端到端目标检测新模型
   - Paper: [MixFormer: End-to-End Tracking with Iterative Mixed Attention](https://arxiv.org/abs/2203.11082)
   - Code: [https://github.com/MCG-NJU/MixFormer](https://github.com/MCG-NJU/MixFormer)
   - Tips: 本文介绍了一篇单目标跟踪(VOT)领域的新工作-基于 transformer 的简洁的端到端模型 MixFormer，该工作已经被CVPR 2022收录。该工作打破了传统的跟踪范式，通过模板与测试样本混合的backbone加上一个简单的回归头直接出跟踪结果，并且不使用框的后处理、多尺度特征融合策略等。

<div align=center><img src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfo2EXh31yIPtb0f44V0uEcUsq3BQV6UXD3cNGh9iaibY0icJB0rvK9vDxeBT2BBAy2zaGnf5OnBQmtyg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>





## :paperclip:  Others

- 由于图片权限问题，[GitHub](https://github.com/xiaoxuebajie/dairly_learning)是完整版，可以点点 star
- 星标的数量是与个人相关程度，不代表文章内容的好坏
- 关注我的[个人网站](http://www.cvbds.cn/)
- 关注我的[CSDN](https://blog.csdn.net/xiaoxuebajie)博客
- 关注我的[哔哩哔哩](https://space.bilibili.com/424394389)
- 关注我的公众号CV伴读社

<div align=center><img src="https://img-blog.csdnimg.cn/202005031406335.jpg" style='zoom:80%'>
</div>