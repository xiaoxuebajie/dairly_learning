# 公众号内容拓展学习笔记（2022.5.25）

------



## :paperclip:  今日要点

1. [CVPR 2022 | FAIR提出MaskFeat：自监督视觉预训练新方法！灵感之一来自16年前CVPR论文](https://mp.weixin.qq.com/s/dU14B0Hmary-q_2Qg278oA)         :star::star:
   - Abstract: FAIR提出MaskFeat：自监督视觉预训练新方法！
   - Paper: [Masked Feature Prediction for Self-Supervised Visual Pre-Training](https://arxiv.org/abs/2112.09133)
   - Tips: MaskFeat最核心的改变就是将MAE对图像像素（pixel）的直接预测，替换成对图像的方向梯度直方图（HOG）的预测。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb3hicLBCn12AYLhQIAZboH07MmVn00upqukxPe7zGrFGaBYJe4SmIH7uX8CMjsJGGWyRAAWhobTqsA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>


2. [GAN掉人脸识别系统？GAN模型「女扮男装」](https://mp.weixin.qq.com/s/SC7iEVfyI2G69OJc4IryEQ)       :star::star:
   - Abstract: GAN掉人脸识别系统？GAN模型「女扮男装」
   - Paper: [Face Verification Bypass](https://arxiv.org/pdf/2203.15068.pdf)
   - Tips: 用对抗生成网络GAN生成一个面部图像来模仿目标人脸，看看人脸识别系统能否正确验证。因为人脸的关键特征信息都保留了下来，所以论文的结果显示，生成的人脸图像仍然可以通过人脸验证。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb17O0YMJxNwVpWdMFA5d6YhyL86sGPFKoLwC4sJASB96xcjXlsH2EOmUiab1s0oiciciaQQGWqOXbSSQA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>

3. [CVPR 2022 | TCTrack: 用于空中跟踪的时序信息框架](https://mp.weixin.qq.com/s/4UgyAwg1MJJH9Aajl35q9w)       :star::star:
   - Abstract: TCTrack: 用于空中跟踪的时序信息框架
   - Paper: [TCTrack: Temporal Contexts for Aerial Tracking](https://arxiv.org/abs/2203.01885)
   - Code: [https://github.com/vision4robotics/TCTrack](https://github.com/vision4robotics/TCTrack)
   - Tips: TCTrack通过特征维度及相似度图维度连续整合时序信息。在特征提取过程中，作者通过使用改进的Online TAdaConv在特征维度高效引入时序信息；而在特征图维度，本文使用了更加高效的时序信息策略，通过不断积累的时序信息修正特征图。
<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/BJbRvwibeSTubGWibia4L1a3j2oAK5IXDrIfmLMhUvtPibXlSaTbicOIhC8zWpexPvPRhvYos8kTlmSpmktcxKaGLfA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>

4. [CVPR 2022 | 清华开源DAT：具有可变形注意力的视觉Transformer](https://mp.weixin.qq.com/s/GydO8rBctduBbVcUKcdIRg)       :star::star:
   - Abstract: 清华开源DAT：具有可变形注意力的视觉Transformer
   - Paper: [Vision Transformer with Deformable Attention](https://arxiv.org/abs/2201.00520)
   - Code: [https://github.com/LeapLabTHU/DAT](https://github.com/LeapLabTHU/DAT)
   - Tips: 本文提出了一种新的可变形的自注意力模块，该模块以数据依赖的方式选择了自注意力中的key和value对的位置。这种灵活的方案使自注意力模块能够聚焦于相关区域并捕获更多信息。在此基础上，提出了一种可变形注意力Transformer(Deformable Attention Transformer)模型，该模型具有可变形注意力，适用于图像分类和密集预测任务。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/5ooHoYt0tgkqhY0ZfBzXZuRNCg3boLickg7ZRvQOoq0bWhobg10evDISHQOdwmWyCBytiaHW77pql15koT68ibJ3A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>

5. [新加坡国立大学&哈工大提出《Incremental-DETR》，基于自监督学习的增量 Few-Shot 目标检测，性能SOTA！](https://mp.weixin.qq.com/s/8EAxkoDxJXzggS12PJx9rg)       :star::star:
   - Abstract: 新加坡国立大学&哈工大提出《Incremental-DETR》，基于自监督学习的增量 Few-Shot 目标检测
   - Paper: [Incremental-DETR: Incremental Few-Shot Object Detection via Self-Supervised Learning](https://arxiv.org/abs/2205.04042)
   - Tips: 在本文中，作者提出了一种新的增量few-shot目标检测框架：针对更具挑战性和现实性的场景，在训练数据集中没有来自基类的样本，只有来自新类的少量样本。作者提出使用两阶段微调策略和自监督学习来保留基类的知识，并学习更好的泛化表示。然后，作者利用知识提取策略，使用新类中的少量样本，将知识从基础转移到新模型。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/BJbRvwibeSTvNwqE03o5I178lF4gxoOm3pp7em23Xtk8DJG0KUk3XocicyNvUGY8acbwfYCuQ7GlF64sjKhB5K6g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>

6. [大规模模型训练tricks集锦](https://mp.weixin.qq.com/s/p99u10YOODDmZQPN0lc03w)       :star::star:
   - Abstract: 大规模模型训练tricks集锦
   - Tips: 大规模模型训练其实就是在和计算、存储和通信玩的过程，所以作者列一下跟这些相关的文章。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_jpg/5fknb41ib9qHGHDgutwHQTnMn7RfH4eANVKkSoMy2leXmRI2Y7NPJqtkIsHYqY9qQibS3WxR17ibIGu3S5icvXCRSA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>




## :paperclip:  Others

- 由于图片权限问题，[GitHub](https://github.com/xiaoxuebajie/dairly_learning)是完整版，可以点点 star
- 星标的数量是与个人相关程度，不代表文章内容的好坏
- 关注我的[个人网站](http://www.cvbds.cn/)
- 关注我的[CSDN](https://blog.csdn.net/xiaoxuebajie)博客
- 关注我的[哔哩哔哩](https://space.bilibili.com/424394389)
- 关注我的公众号CV伴读社

<div align=center><img src="https://img-blog.csdnimg.cn/202005031406335.jpg" style='zoom:80%'>
</div>