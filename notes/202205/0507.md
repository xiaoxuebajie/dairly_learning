# 公众号内容拓展学习笔记（2022.5.7）

------



## :paperclip:  今日要点

1. [CVPR 2022｜未标注视频也能训练目标检测？微软提出时空目标蒸馏框架STUD](https://mp.weixin.qq.com/s/JNfHqH09byZ5JSfQ8BuUhA)         :star::star:
   - Abstract: 微软提出时空目标蒸馏框架STUD
   - Paper: [Unknown-Aware Object Detection: Learning What You Don't Know from Videos in the Wild](https://arxiv.org/abs/2203.03800)
   - Code: [https://github.com/deeplearning-wisc/stud](https://github.com/deeplearning-wisc/stud)
   - Tips:  本文来自威斯康星大学麦迪逊分校和微软研究院，提出了一种时空未知目标蒸馏框架（Spatial-Temporal Unknown Distillation，STUD），其可以在大量未标注视频中提取未知目标，并对原有目标检测模型的决策边界进行规范化。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/ibaXaPIy7jV10vZ2WZibxcTF80j2qX8URQoFXtHsEHeOoGLd2KHNnQNhTDG736J7Z7iblIQ43hxcnzUyIzb27UJkA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>




2. [只需要十分之一数据，就能通关四大视觉任务，居然还开源了！](https://mp.weixin.qq.com/s/7R8DgOvqxlcjCYhgAqrd2g)       :star::star:
   - Abstract: OpenGVLab开源超高性能预训练模型，节省90%数据量！分类、目标检测、语义分割、深度估计，四大任务一网打尽！
   - Paper: [INTERN: A New Learning Paradigm Towards General Vision](arxiv.org/abs/2111.08687)
   - Code: [https://github.com/opengvlab](https://github.com/opengvlab)
   - Tips: 上海人工智能实验室联合商汤科技、香港中文大学、上海交通大学发布通用视觉技术体系“书生”INTERN，一套持续学习框架，用于系统化解决当下人工智能视觉领域中存在的任务通用、场景泛化和数据效率等一系列瓶颈问题。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/cNFA8C0uVPsNMYMVjuI9CJjjx6SibFTbnjibEqgN3dA0DWfUXE9zhRL45mZIHydozklgmXMeiaXetic7g8UAzSb2DA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>



3. [EdgeFormer: 向视觉 Transformer 学习，构建一个比 MobileViT 更好更快的卷积网络](https://mp.weixin.qq.com/s/R53VtsmH7sAkPQ6UXvw4LQ)       :star::star:
   - Abstract: EdgeFormer: 向视觉 Transformer 学习，构建一个比 MobileViT 更好更快的卷积网络
   - Paper: [EdgeFormer: Improving Light-weight ConvNets by Learning from Vision Transformers](https://arxiv.org/abs/2203.03952)
   - Code: [https://github.com/hkzhang91/EdgeFormer](https://github.com/hkzhang91/EdgeFormer)
   - Tips: 本文提出的核心算子，即 global circular convolution (GCC)，是一个卷积操作，但是会引入位置嵌入，同时还具有全局的感受野。另一个改进点是使用提出的 GCC 和 SE 操作构建了类似于 Vision Transformer 的基础操作单元。借助于 SE 引入了样本相关的注意力机制。
<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/V2E1ll6kaTWg50IKKplAickSELFnEHvzJmgyjsLn4sNc9WppZiaRhMzzOiawwU0frhLxA9GyjACfypVCmOaFC1hGg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>



4. [LSTM在CV领域杀出一条血路！Sequencer：完美超越Swin与ConvNeXt等前沿算法](https://mp.weixin.qq.com/s/gv9TTnk84AOGwECUBqRjsg)       :star::star:
   - Abstract: Sequencer：完美超越Swin与ConvNeXt等前沿算法
   - Paper: [Sequencer: Deep LSTM for Image Classification](https://arxiv.org/abs/2205.01972)
   - Tips: 本文提出Sequencer，一个全新且具有竞争性的架构，可以替代ViT，为分类问题提供了一个全新的视角。作者还提出了一个二维的Sequencer模块，其中一个LSTM被分解成垂直和水平的LSTM，以提高性能。

<div align=center><img src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfraib0TMLNymB7s3Fcpd7KSHxZDX3ib5Wa3mUnhAXhYb7omhNMiczfhjMPKlCoqsLqQBcMYrQDzKktXA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>



5. [视觉Transformer的复仇！Meta AI提出DeiT III：ViT训练的全新baseline](https://mp.weixin.qq.com/s/hhSY5qv6mCDlBmNgu-VWUw)       :star::star::star::star:
   - Abstract: Meta AI提出DeiT III：ViT训练的全新baseline
   - Paper: [DeiT III: Revenge of the ViT](https://arxiv.org/abs/2204.07118)
   - Tips: 本文提出了训练视觉 Transformer（ViT）的三种数据增强方法：灰度、过度曝光、高斯模糊，以及一种简单的随机剪枝方法 (SRC)。实验结果表明，这些新方法在效果上大大优于 ViT 此前的全监督训练方法。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9JnGBvutVGOzsvCtYyHib8Zicfr3RBYjWFDMKczdE5X0X5Jx2DW3xlYj2gDogHWNDXVgFnicVic2YrMA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>



6. [引入特征空间，显著降低计算量：双边局部注意力ViT性能媲美全局注意力](https://mp.weixin.qq.com/s/487MBvfsy6s4chLCTD3MYQ)       :star::star:
   - Abstract: 引入特征空间，显著降低计算量：双边局部注意力ViT性能媲美全局注意力
   - Paper: [BOAT: Bilateral Local Attention Vision Transformer](https://arxiv.org/pdf/2201.13027v1.pdf)
   - Code: [https://github.com/mahaoyuHKU/pytorch-boat](https://github.com/mahaoyuHKU/pytorch-boat)
   - Tips: 作者提出了双边局部注意力 ViT （简称 BOAT)，把特征空间局部注意力模块加入到现有的基于窗口的局部注意力视觉 Transformer 模型中，作为图像空间局部注意力的补充，大大提升了针对远距离特征依赖的建模能力，在几个基准数据集上的大量实验表明结合了特征空间局部注意力的模型明显优于现有的 ConvNet 和 ViT 模型。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicLY3WEsU1F8x72suaCqRMdsV0Hrp9gbQBsd66fEIo6Kib4MD4rt7SI0DtfQ5XJsmI3Fsqb7Bh2odw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>




## :paperclip:  Others

- 由于图片权限问题，[GitHub](https://github.com/xiaoxuebajie/dairly_learning)是完整版，可以点点 star
- 星标的数量是与个人相关程度，不代表文章内容的好坏
- 关注我的[个人网站](http://www.cvbds.cn/)
- 关注我的[CSDN](https://blog.csdn.net/xiaoxuebajie)博客
- 关注我的[哔哩哔哩](https://space.bilibili.com/424394389)
- 关注我的公众号CV伴读社

<div align=center><img src="https://img-blog.csdnimg.cn/202005031406335.jpg" style='zoom:80%'>
</div>