# 公众号内容拓展学习笔记（2022.5.9）

------



## :paperclip:  今日要点

1. [CVPR 2022 | 北大、腾讯提出文字logo生成模型，脑洞大开](https://mp.weixin.qq.com/s/UDkE-tHvahrAVGZGn7_6vQ)         :star::star:
   - Abstract: 北大、腾讯提出文字logo生成模型
   - Paper: [Aesthetic Text Logo Synthesis via Content-aware Layout Inferring](https://arxiv.org/abs/2204.02701)
   - Code: [https://github.com/yizhiwang96/TextLogoLayout](https://github.com/yizhiwang96/TextLogoLayout)
   - Tips:  本模型基于 Conditional GAN 来生成文字 logo，创新性地使用双判别器结构（序列判别器和图像判别器），对字形的轨迹序列和整体 logo 图像分别做判别；同时借助可微分拼接(Differentiable Composition)，构建位置坐标到 logo 图像的可微分渲染过程。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibqentvzSlBUfic9MTjYhj8jfgaicBOqeFdYxVSibrbgJ9gZxJQfvpS57UibZ7bzGYViaLt7ojZah9SSNg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>



2. [CVPR2022 Oral：GAN监督的密集视觉对齐，代码开源](https://mp.weixin.qq.com/s/YI3CRFeSQwvnfYWFY2OQMg)       :star::star:
   - Abstract: GAN监督的密集视觉对齐，代码开源
   - Paper: [GAN-Supervised Dense Visual Alignment](https://arxiv.org/abs/2112.05143)
   - Code: [https://www.github.com/wpeebles/gangealing](https://www.github.com/wpeebles/gangealing)
   - Tips: 在该论文中作者提出了一种用于端到端联合学习的GAN生成数据的框架。受到经典方法的启发，论文中作者联合训练一个空间变换器，将随机样本从基于未对齐数据训练的GAN映射到共同的、联合学习的目标模式。

<video id="video" controls=""src="http://mpvideo.qpic.cn/0bc3iiaa4aaa4qao57dpnnrfaqwdbzbaadqa.f10003.mp4?dis_k=85b8fb229280fbf356d366dec7e53593&dis_t=1652058067&vid=wxv_2380802778538688513&format_id=10003&support_redirect=0&mmversion=false" preload="none">


3. [ACL'22 | 陈丹琦提出CoFi模型剪枝，加速10倍，精度几乎无损](https://mp.weixin.qq.com/s/0VO036qHI8JYfYu_r-3Tgw)       :star::star:
   - Abstract: 陈丹琦提出CoFi模型剪枝，加速10倍，精度几乎无损
   - Paper: [Structured Pruning Learns Compact and Accurate Models](https://arxiv.org/pdf/2204.00408.pdf)
   - Code: [https://github.com/princeton-nlp/CoFiPruning](https://github.com/princeton-nlp/CoFiPruning)
   - Tips: 作者提出的结构化剪枝方法 CoFi 在几乎没有太多精度损失的情况下，达到了 10 倍以上的加速比，同时，和常规的蒸馏做法相比，避免了因使用大量无标签数据预训练模型而带来的训练成本过高的问题，按作者的话来说，该方法可以是蒸馏的一个有效替代品。
<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/5fknb41ib9qEqBFibgRXqh8zdwZ3TIbOqUWO2GJpFnStnqzicibsUkfoUhvhL2Evl3RvAsxBa97Hk5nBTZj6gvRTmA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>




4. [MAGIC: 一个即插即用、无需训练的图像-文本生成框架](https://mp.weixin.qq.com/s/bCeALWVmPZnNvfMN7xOrLQ)       :star::star:
   - Abstract: MAGIC: 一个即插即用、无需训练的图像-文本生成框架
   - Paper: [LanguageModels Can See: Plugging Visual Controls in Text Generation](https://arxiv.org/abs/2205.02655)
   - Code: [https://github.com/yxuansu/MAGIC](https://github.com/yxuansu/MAGIC)
   - Tips: 本文提出了一个全新的MAGIC框架。该框架可以使用图片模态的信息指导预训练语言模型完成一系列跨模态生成任务与其他方法不同的是，MAGIC框架无需多模态的训练数据，只需利用现成的语言模型和图文匹配模型就能够以zero-shot的方式高质量地完成多模态生成任务。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/nJZZib3qIQW7M5v7ChRYa7Gol6z2VC4WficogWcOwms9VA7l0icqZkqbjn9gKsKAxbohU242ribtCDvjCxrW1pfic6g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>




5. [多模态理解与生成，西南交大&MSRA提出统一的"视频和语言预训练"模型：UniVL！](https://mp.weixin.qq.com/s/P3WiKQmboTrgad2JzIAvgA)       :star::star::star::star:
   - Abstract: 西南交大&MSRA提出统一的"视频和语言预训练"模型：UniVL
   - Paper: [UniVL: A Uniﬁed Video and Language Pre-Training Model for Multimodal Understanding and Generation](https://arxiv.org/abs/2002.06353)
   - Code: [https://github.com/microsoft/UniVL](https://github.com/microsoft/UniVL)
   - Tips: 本文提出了一种基于自监督学习的大规模视频语言表示方法UniVL。UniVL设计有四个模块和五个目标，用于视频语言理解和生成任务。它是一个灵活的模型，适用于大多数多模态下游任务，同时考虑效率和有效性。

<div align=center><img src="https://mmbiz.qpic.cn/mmbiz_png/BJbRvwibeSTtUnlibcGGfdY7DlpgcicSf5iafNHcZJCCm1DuQDTLXCxtQvPpXm09CzxyroZCup5Uh4xZ2sM8RWDwsA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>




6. [小样本学习只是一场学术界自嗨吗？](https://mp.weixin.qq.com/s/GyRxJ47HDpz2JZatWxPnOg)       :star::star:
   - Abstract: 小样本学习只是一场学术界自嗨吗？
   - Tips: 这两年看见很多人，都在批评few-shot learning，觉得是学术界在自high，思考良久，感觉有必要给这个领域正个名～（注意，本文仅关注few-shot image classification）

<div align=center><img src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfqMsywC0jhRXchnH1jh4dIsdgbCRuYGuhPkQgcopJ4mS9Veerq4pD5MicXHBDntrAkvMtuv7ebGicOQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" style='zoom:100%'>
</div>



## :paperclip:  Others

- 由于图片权限问题，[GitHub](https://github.com/xiaoxuebajie/dairly_learning)是完整版，可以点点 star
- 星标的数量是与个人相关程度，不代表文章内容的好坏
- 关注我的[个人网站](http://www.cvbds.cn/)
- 关注我的[CSDN](https://blog.csdn.net/xiaoxuebajie)博客
- 关注我的[哔哩哔哩](https://space.bilibili.com/424394389)
- 关注我的公众号CV伴读社

<div align=center><img src="https://img-blog.csdnimg.cn/202005031406335.jpg" style='zoom:80%'>
</div>